{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5337ddfe",
   "metadata": {},
   "source": [
    "# Generaci√≥n de Contenido de Marketing con IA para ESU Analytics  \n",
    "\n",
    "Este cuaderno de Jupyter presenta una **prueba de concepto (POC)** orientada a la generaci√≥n de contenido de marketing digital para **ESU Analytics**, una consultora especializada en acompa√±ar a PyMEs de los sectores manufacturero y comercial en la profesionalizaci√≥n de su estrategia de datos.  \n",
    "\n",
    "La POC tiene como prop√≥sito:  \n",
    "- **Demostrar** c√≥mo aplicar t√©cnicas de *Fast Prompting* para optimizar la creaci√≥n de publicaciones en LinkedIn.  \n",
    "- **Experimentar** con diferentes configuraciones de prompts para evaluar su impacto en la claridad, relevancia y consistencia del contenido generado.  \n",
    "- **Controlar costos y eficiencia**, implementando l√≠mites de tokens y buenas pr√°cticas de uso del modelo `gpt-4o-mini` de OpenAI.  \n",
    "- **Garantizar trazabilidad**, exportando cada ejecuci√≥n con su *input* (brief y prompt generado) y *output* (texto final y m√©tricas de tokens).  \n",
    "\n",
    "De esta manera, se busca comprobar si las t√©cnicas aprendidas permiten mejorar la propuesta de soluci√≥n planteada en la etapa anterior, asegurando un flujo de generaci√≥n de contenido **m√°s √°gil, profesional y reproducible**, que apoye la estrategia de visibilidad digital de ESU Analytics.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413a188",
   "metadata": {},
   "source": [
    "## Par√°metros del brief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9549cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√°metros del brief que gu√≠an el prompt\n",
    "\n",
    "# Ejemplo comentado de valores posibles:\n",
    "# brief_tema = \"C√≥mo ordenar datos de ventas para tomar decisiones mejor informadas\",\n",
    "brief_tema = \"C√≥mo ordenar datos de ventas para tomar decisiones mejor informadas\",\n",
    "# brief_objetivo = \"Educar y generar inter√©s por un diagn√≥stico inicial\",\n",
    "brief_objetivo = \"Educar y generar inter√©s por un diagn√≥stico inicial\",\n",
    "# brief_audiencia = \"Due√±os de PyMEs retail y gerentes comerciales\",\n",
    "brief_audiencia = \"Due√±os de PyMEs retail y gerentes comerciales\",\n",
    "# brief_insight = \"Sin datos limpios y consistentes, el margen se vuelve impredecible\",\n",
    "brief_insight = \"Sin datos limpios y consistentes, el margen se vuelve impredecible\",\n",
    "# brief_evidencia = \"Planillas dispersas generan errores; unificarlas reduce reprocesos\",\n",
    "brief_evidencia = \"Planillas dispersas generan errores; unificarlas reduce reprocesos\",\n",
    "# brief_cta = \"Agend√° una consulta gratuita para revisar tu caso\",\n",
    "brief_cta = \"Agend√° una consulta gratuita para revisar tu caso\",\n",
    "# brief_restricciones = \"m√°x. 110 palabras, sin hashtags ni emojis\"\n",
    "brief_restricciones = \"m√°x. 110 palabras, sin hashtags ni emojis\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c6a78",
   "metadata": {},
   "source": [
    "## üì¶ 0. Setup (variables y helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cbec8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer√≠as y configurar cliente\n",
    "import os, time, json\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca7246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar la API Key desde .env\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Tokenizer para modelos GPT-4o/mini\n",
    "ENCODING_NAME = \"o200k_base\"\n",
    "enc = tiktoken.get_encoding(ENCODING_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676bc18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares\n",
    "def truncate_by_tokens(text: str, max_tokens: int) -> str:\n",
    "    tokens = enc.encode(text)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "    return enc.decode(tokens[:max_tokens])\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def now_ms():\n",
    "    return round(time.time() * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c24d6a",
   "metadata": {},
   "source": [
    "## üß™ 1. Prompt builder (variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d33a570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Eres un creador de contenido para ESU Analytics, consultora que ayuda a PyMEs de Manufactura y Comercio a profesionalizar su estrategia de datos (ventas, stock, estrategia comercial).\n",
    "Tu prioridad: claridad, utilidad pr√°ctica para due√±os/gerentes PyME en Argentina, tono profesional y cercano, y una llamada a la acci√≥n concreta.\n",
    "Responde siempre en espa√±ol de Buenos Aires neutral.\n",
    "\"\"\"\n",
    "\n",
    "USER_TEMPLATE = \"\"\"Tarea: Escribe un post de LinkedIn.\n",
    "\n",
    "Brief:\n",
    "- Tema: {tema}\n",
    "- Objetivo del post: {objetivo}\n",
    "- Audiencia: {audiencia}\n",
    "- √Ångulo/Insight: {insight}\n",
    "- Evidencia/ejemplo breve (opcional): {evidencia}\n",
    "- CTA: {cta}\n",
    "- Restricciones: {restricciones}\n",
    "\n",
    "Formato deseado:\n",
    "- 3‚Äì5 l√≠neas, con una idea central y un cierre con CTA.\n",
    "- Incluye 1 micro-lista de 3 √≠tems si aporta claridad.\n",
    "- No uses hashtags en esta versi√≥n.\n",
    "\n",
    "Entrega:\n",
    "Devuelve SOLO un objeto JSON con:\n",
    "{{\n",
    " \"titulo\": \"...\",\n",
    " \"cuerpo\": \"...\",\n",
    " \"cta\": \"...\",\n",
    " \"palabras\": <int>\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def build_user_prompt(tema, objetivo, audiencia, insight, evidencia, cta, restricciones):\n",
    "    return USER_TEMPLATE.format(\n",
    "        tema=tema,\n",
    "        objetivo=objetivo,\n",
    "        audiencia=audiencia,\n",
    "        insight=insight,\n",
    "        evidencia=evidencia or \"‚Äî\",\n",
    "        cta=cta,\n",
    "        restricciones=restricciones\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069525f",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 2. Llamada a OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9204878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_post(brief, \n",
    "                  max_input_tokens=800, \n",
    "                  max_output_tokens=220, \n",
    "                  temperature=0.3):\n",
    "    user_prompt = build_user_prompt(**brief)\n",
    "    user_prompt = truncate_by_tokens(user_prompt, max_input_tokens)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    t0 = now_ms()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        max_tokens=max_output_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    dt = now_ms() - t0\n",
    "\n",
    "    text = resp.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except Exception:\n",
    "        data = {\"raw\": text}\n",
    "\n",
    "    meta = {\n",
    "        \"elapsed_ms\": dt,\n",
    "        \"input_tokens_est\": count_tokens(SYSTEM_PROMPT) + count_tokens(user_prompt),\n",
    "        \"output_tokens_est\": count_tokens(text),\n",
    "        \"total_tokens_est\": count_tokens(SYSTEM_PROMPT) + count_tokens(user_prompt) + count_tokens(text),\n",
    "        \"api\": \"chat.completions\",\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"max_output_tokens\": max_output_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    # üëá devolvemos tambi√©n el prompt de usuario final para trazabilidad\n",
    "    return data, meta, user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d155da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Helpers de exportaci√≥n\n",
    "# ============================\n",
    "import os, re, json, base64\n",
    "from datetime import datetime\n",
    "\n",
    "def ensure_export_dir(path=\"Exportado\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def next_post_id(path=\"Exportado\"):\n",
    "    \"\"\"\n",
    "    Escanea 'Exportado' y devuelve el pr√≥ximo ID tipo '001', '002', ...\n",
    "    Detecta archivos 'postXYZ.json' o 'postXYZ.md'\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(path):\n",
    "        return \"001\"\n",
    "    max_n = 0\n",
    "    for fname in os.listdir(path):\n",
    "        m = re.match(r\"post(\\d{3})\\.(json|md)$\", fname, re.IGNORECASE)\n",
    "        if m:\n",
    "            n = int(m.group(1))\n",
    "            if n > max_n:\n",
    "                max_n = n\n",
    "    return f\"{max_n+1:03d}\"\n",
    "\n",
    "def export_run(data, meta, brief, user_prompt, export_dir=\"Exportado\"):\n",
    "    \"\"\"\n",
    "    Guarda:\n",
    "      - postXYZ.json con input, prompts, output y meta\n",
    "      - postXYZ.md con texto listo para publicar\n",
    "    \"\"\"\n",
    "    ensure_export_dir(export_dir)\n",
    "    run_id = next_post_id(export_dir)\n",
    "    ts = datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "    # Paquete JSON completo (para trazabilidad)\n",
    "    payload = {\n",
    "        \"timestamp\": ts,\n",
    "        \"project\": \"ESU Analytics - Marketing con IA\",\n",
    "        \"inputs\": {\n",
    "            \"brief\": brief,\n",
    "            \"system_prompt\": SYSTEM_PROMPT,\n",
    "            \"user_prompt\": user_prompt\n",
    "        },\n",
    "        \"output\": data,\n",
    "        \"meta\": meta\n",
    "    }\n",
    "\n",
    "    json_path = os.path.join(export_dir, f\"post{run_id}.json\")\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Markdown ‚Äúlisto para publicar‚Äù\n",
    "    md_lines = []\n",
    "    md_lines.append(f\"# {data.get('titulo','(Sin t√≠tulo)')}\")\n",
    "    md_lines.append(\"\")\n",
    "    cuerpo = data.get(\"cuerpo\") or data.get(\"raw\") or \"\"\n",
    "    md_lines.append(cuerpo)\n",
    "    md_lines.append(\"\")\n",
    "    if \"cta\" in data and data[\"cta\"]:\n",
    "        md_lines.append(f\"**CTA:** {data['cta']}\")\n",
    "        md_lines.append(\"\")\n",
    "    if \"palabras\" in data:\n",
    "        md_lines.append(f\"_Palabras: {data['palabras']}_\")\n",
    "        md_lines.append(\"\")\n",
    "    md_lines.append(f\"<!-- post ID: {run_id} | {ts} | modelo: {meta.get('model')} | tokens aprox: {meta.get('total_tokens_est')} -->\")\n",
    "\n",
    "    md_path = os.path.join(export_dir, f\"post{run_id}.md\")\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(md_lines))\n",
    "\n",
    "    return {\"json\": json_path, \"md\": md_path, \"id\": run_id}\n",
    "\n",
    "# ============================\n",
    "# Helpers para generar infograf√≠a con OpenAI Images\n",
    "# ============================\n",
    "def _extract_key_points_for_image(post, max_points=3):\n",
    "    \"\"\"\n",
    "    Extrae 2‚Äì3 ideas clave del 'cuerpo' para orientar la infograf√≠a\n",
    "    (sin texto literal; solo conceptos visuales).\n",
    "    \"\"\"\n",
    "    cuerpo = (post.get(\"cuerpo\") or post.get(\"raw\") or \"\").strip()\n",
    "    # tomar frases cortas\n",
    "    parts = [p.strip(\" -‚Ä¢*‚Äî‚Äì\\n\\r\\t \") for p in re.split(r\"[.\\n]\", cuerpo) if p.strip()]\n",
    "    parts = [p for p in parts if len(p.split()) >= 3][:max_points]\n",
    "    if not parts:\n",
    "        parts = [\"Orden de datos\", \"Decisiones informadas\", \"Control del negocio\"]\n",
    "    return parts[:max_points]\n",
    "\n",
    "def build_image_prompt_from_post(post, brief):\n",
    "    \"\"\"\n",
    "    Crea un prompt descriptivo para gpt-image-1,\n",
    "    evitando pedir texto rendereado en la imagen.\n",
    "    \"\"\"\n",
    "    titulo = post.get(\"titulo\", \"Contenido para PyMEs\")\n",
    "    puntos = _extract_key_points_for_image(post, max_points=3)\n",
    "    cta = post.get(\"cta\", \"Agend√° una consulta\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Infograf√≠a minimalista y profesional, orientaci√≥n horizontal, para una publicaci√≥n en LinkedIn de una consultora que ayuda a PyMEs a profesionalizar su estrategia de datos.\n",
    "Estilo visual: limpio, geom√©trico, iconogr√°fico, fondo claro, 2‚Äì3 colores sobrios (tonos fr√≠os/grises). Sin personajes. Con texto embebido. Sin logotipos.\n",
    "Composici√≥n: bloques/tiles con √≠conos simples (tablero/indicadores, planillas/tabla, caja de producto/stock, flechas de mejora).\n",
    "Transmitir conceptualmente:\n",
    "- Idea central: {titulo}\n",
    "- Puntos clave: {\", \".join(puntos)}\n",
    "- Llamado a la acci√≥n (concepto, no texto): {cta}\n",
    "Sensaci√≥n buscada: orden, claridad, decisi√≥n informada, control del negocio.\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "def generate_infographic_openai_image_from_post(\n",
    "    client, post, brief, export_dir=\"Exportado\", run_id=\"000\",\n",
    "    primary_size=\"1536x1024\", fallback_size=\"1024x1024\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera una imagen con gpt-image-1 a partir del post generado.\n",
    "    Intenta tama√±o horizontal (1536x1024) y si falla, usa 1024x1024.\n",
    "    Devuelve la ruta al PNG guardado.\n",
    "    \"\"\"\n",
    "    prompt_img = build_image_prompt_from_post(post, brief)\n",
    "\n",
    "    def _gen_and_save(size):\n",
    "        result = client.images.generate(\n",
    "            model=\"gpt-image-1\",\n",
    "            prompt=prompt_img,\n",
    "            size=size,\n",
    "            n=1\n",
    "        )\n",
    "        b64 = result.data[0].b64_json\n",
    "        img_bytes = base64.b64decode(b64)\n",
    "        out_path = os.path.join(export_dir, f\"post{run_id}_infografia_ai.png\")\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(img_bytes)\n",
    "        return out_path\n",
    "\n",
    "    try:\n",
    "        return _gen_and_save(primary_size)\n",
    "    except Exception as e:\n",
    "        print(f\"[Aviso] No se pudo generar en {primary_size}. Fallback a {fallback_size}. Detalle: {e}\")\n",
    "        return _gen_and_save(fallback_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8347e1a",
   "metadata": {},
   "source": [
    "## üß™ 3. Ejecuci√≥n A/B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08019d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "brief = {\n",
    "    \"tema\": brief_tema,\n",
    "    \"objetivo\": brief_objetivo,\n",
    "    \"audiencia\": brief_audiencia,\n",
    "    \"insight\": brief_insight,\n",
    "    \"evidencia\": brief_evidencia,\n",
    "    \"cta\": brief_cta,\n",
    "    \"restricciones\": brief_restricciones\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a0aea",
   "metadata": {},
   "source": [
    "## üìä 4. Micro-evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cfa0307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== POST GENERADO =====\n",
      "{\n",
      "  \"titulo\": \"C√≥mo ordenar datos de ventas para decisiones m√°s informadas\",\n",
      "  \"cuerpo\": \"Sin datos limpios y consistentes, el margen de tu negocio se vuelve impredecible. Para evitar errores y mejorar la toma de decisiones, considera estos pasos: \\n1. Unifica tus planillas de ventas. \\n2. Establece un formato est√°ndar. \\n3. Realiza auditor√≠as peri√≥dicas. \\nOrdenar tus datos no solo reduce reprocesos, sino que tambi√©n potencia tu estrategia comercial.\",\n",
      "  \"cta\": \"Agend√° una consulta gratuita para revisar tu caso.\",\n",
      "  \"palabras\": 104\n",
      "}\n",
      "\n",
      "===== M√âTRICAS =====\n",
      "{'elapsed_ms': 3547, 'input_tokens_est': 312, 'output_tokens_est': 136, 'total_tokens_est': 448, 'api': 'chat.completions', 'model': 'gpt-4o-mini', 'max_output_tokens': 220, 'temperature': 0.2}\n",
      "\n",
      "===== ARCHIVOS GUARDADOS =====\n",
      "{'json': 'Exportado\\\\post002.json', 'md': 'Exportado\\\\post002.md', 'id': '002'}\n"
     ]
    }
   ],
   "source": [
    "post, meta, user_prompt = generate_post(brief, temperature=0.2, max_output_tokens=220)\n",
    "paths = export_run(post, meta, brief, user_prompt, export_dir=\"Exportado\")\n",
    "\n",
    "print(\"\\n===== POST GENERADO =====\")\n",
    "print(json.dumps(post, ensure_ascii=False, indent=2))\n",
    "print(\"\\n===== M√âTRICAS =====\")\n",
    "print(meta)\n",
    "print(\"\\n===== ARCHIVOS GUARDADOS =====\")\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fcacb7",
   "metadata": {},
   "source": [
    "## üñºÔ∏è 5. Generaci√≥n de infograf√≠a (OpenAI Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c27cb1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== INFOGRAF√çA =====\n",
      "Infograf√≠a guardada en: Exportado\\post002_infografia_ai.png\n"
     ]
    }
   ],
   "source": [
    "# Usamos el mismo ID que se gener√≥ al exportar el post para mantener trazabilidad\n",
    "export_dir = \"Exportado\"\n",
    "run_id = paths[\"id\"]\n",
    "\n",
    "# Genera una infograf√≠a horizontal (1536x1024) y si no es posible, cae a 1024x1024\n",
    "img_path = generate_infographic_openai_image_from_post(\n",
    "    client=client,\n",
    "    post=post,\n",
    "    brief=brief,\n",
    "    export_dir=export_dir,\n",
    "    run_id=run_id,\n",
    "    primary_size=\"1536x1024\",\n",
    "    fallback_size=\"1024x1024\"\n",
    ")\n",
    "\n",
    "print(\"\\n===== INFOGRAF√çA =====\")\n",
    "print(\"Infograf√≠a guardada en:\", img_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed954b5",
   "metadata": {},
   "source": [
    "# Conclusi√≥n y pr√≥ximos pasos  \n",
    "\n",
    "La presente POC demostr√≥ la **viabilidad t√©cnica y estrat√©gica** de utilizar *Fast Prompting* con modelos de IA generativa para asistir en la creaci√≥n de contenido de marketing digital para ESU Analytics.  \n",
    "\n",
    "Los principales logros fueron:  \n",
    "- Generaci√≥n automatizada de publicaciones en LinkedIn con un estilo claro, profesional y enfocado en PyMEs.  \n",
    "- Implementaci√≥n de **l√≠mites de tokens** para optimizar costos de uso del modelo.  \n",
    "- Registro completo de cada ejecuci√≥n (*input*, *output* y m√©tricas), garantizando **trazabilidad y reproducibilidad**.  \n",
    "- Establecimiento de una base flexible que permite experimentar con diferentes configuraciones de prompts y comparar su eficacia.  \n",
    "\n",
    "**Pr√≥ximos pasos sugeridos:**  \n",
    "- Ampliar el set de pruebas con diferentes briefs y audiencias espec√≠ficas.  \n",
    "- Incorporar variantes sistem√°ticas (A/B testing) con cambios en tono, longitud y estructura.   \n",
    "- Evaluar m√©tricas de impacto real en redes (engagement, alcance) para validar la efectividad del contenido generado.  \n",
    "\n",
    "Con estos avances, ESU Analytics podr√° contar con un flujo de producci√≥n de contenido **m√°s √°gil, escalable y orientado a resultados**, potenciando su estrategia de posicionamiento digital y captaci√≥n de clientes.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
